AlphaGo Algorithm

Paper:
http://airesearch.com/wp-content/uploads/2016/01/deepmind-mastering-go.pdf

Overview:

  Train a supervised learning policy (P_expert) on a dataset of expert
  human moves to predict the next move given a board state.

  Train a fast policy (P_fast) in a similar manner but using a smaller
  neural network so that actions can be sampled rapidly during rollouts.

  Train a reinforcement learning policy (P_rl) that is initialized to
  P_expert, but is then adjusted using RL to increase the odds of it choosing
  moves that lead to a win.

  Train a value network (V_rl) using RL to predict the outcome of the game from
  the current board state.

  Use P_rl and V with MCTS to play Go.


P_expert:
  Model: Board state -> n layers (Conv + ReLu) -> Softmax over legal moves
  Optimizer: SGD
  Dataset: state/action pairs for 29.4 million (28.4 mill train, 1 mill test)
    board positions from KGS Go Server (plus precomputed data augmentation of
    using all 8 rotation/reflections for each state-action pair).
  Training:
    * SGD step-size initialized to 0.003, and halved every 80 mill steps.
    * mini-batch size of 16.
    * Updates were applied asynchronously on 50 GPUs.
    * Gradients older than 100 steps were discarded.
    * Training took around 3 weeks for 340 million training steps.

P_fast:
  Same as P_expert, except the model was different and it could run in
  3 microseconds instead of 2 milliseconds.
  Model: Board state -> small pattern features -> a linear layer ->
         Softmax over legal moves

P_tree:
  Similar to as P_tree but with more features. Used to initialize new leaf
  nodes while P_rl is being asynchronously executed on a different thread.


P_rl:
  Same model as P_expert and initialized to the same weights.

  Trained using RL and self play against a randomly selected previous
  iteration of the policy. Randomizing opponents stabilized training by
  preventing overfitting to the current policy.

  Reward is 0 for all non-terminal states, and for terminal states it is:
  1 for win, -1 for loss (ties were not allowed in this version).

  Model is updated using SGD to maximize expected outcome (total reward).

  Training:
    * Played batch of 128 games in parallel using the current policy against
      randomly selected previous iteration of the policy. Randomizing opponents
      stabilized training by preventing overfitting to the current policy.
    * The gradients for state-actions pairs were calculated using REINFORCE
      with V_rl as the baseline value (gradient of log(P_rl)(z - V_rl)).
      (Note: on first pass through the training pipeline, use zero instead of
       V_rl as the baseline).
    * Every 500 iterations, add current parameters to the opponent pool.
    * Trained for 10,000 mini-batches, of 128 games each, using 50 GPUs, for
      one day.

V_rl:
  Similar model as P_rl, expect outputs a single prediction, the outcome of
  the game if played by the P_rl policy given the current board state.

  Trained using SGD to minimize MSE.

  Dataset: state-outcome pairs for 30 million distinct positions, each from a
  separate game. (Note: using states from the same game lead to overfitting
  (the network just memorizing the games), whereas using states all from
  separate games, lead to better generalization.)

  Each game's state-outcome pair was generated by:
  1. Randomly sample U from uniform{4, 450}.
  2. Sample first U moves from P_rl.
  3. Sample a random move from all possible legal next moves.
  4. Sample all remaining moves from P_rl until the game terminates.
  5. Add (s_U+1, z_U+1) to the dataset.

  Training:
    * Identical to P_rl except use the MSE loss.
    * Trained for 50 million mini-batches, of 32 positions each, using 50 GPUs,
      for one week.

Policy Network Architecture:
  Input shape: 19, 19, 48
  Layer 1: Conv 5x5, 192 filters, stride of 1 zero padded -> ReLU
  Layers 2-12: Conv 3x3, 192 filters, stride of 1 zero padded -> RelU
  Layer 13: Conv 1x1, 1 filter, stride of 1 (with a different bias for each
            position) -> Softmax

Value Network Architecture:
  Input shape: 19, 19, 49 (an additional feature for the current color to play)
  Layer 1: Conv 5x5, 192 filters, stride of 1 zero padded -> ReLU
  Layers 2-13: Conv 3x3, 192 filters, stride of 1 zero padded -> RelU
  Layer 13: Conv 1x1, 1 filter, stride of 1 -> ReLU
  Layer 14: Fully connected 256 -> ReLu
  Layer 15: Fully connected 1 -> Tanh

Constants:
  beta (softmax temperature) = 0.67
  lambda (mixing parameter) = 0.5
  n_vl (virtual loss) = 3
  n_thr (expansion threshold) = 40 (initially, adjusted dynamically)
  c_puct (exploration constant) = 5

For details on the state representation, see the paper.


MCTS:

  Each node stores edges for all legal actions from that node.

  Each edge stores:
    P(s, a), prior probability
    N_v(s, a), value function visit count
    W_v(s, a), value function total from all leaf nodes
    N_r(s, a), rollout visit count
    W_r(s, a), rollout total from all leaf nodes
    Q(s, a), mean action value

  MCTS runs a number of simulations asynchronously, where each simulation
  performs these steps in a loop: selection, expansion, evaluation, and backup.

  Selection:
    Start at the root node, then select child nodes using the below selection
    algorithm until a leaf node is found.
    The selection algorithm (a variant of PUCT):
      action = argmax over a of (Q(s, a) + u(s, a))
      Q(s, a) = (1 - lambda) * W_v(s, a) / N_v(s, a) + lambda * W_v(s, a) / N_v(s, a)
      u(s, a) = c_puct * P(s, a) * sqrt(sum(N_r(s, *))) / (1 + N_r(s, a))

  Expansion:
    1. If the visit count of the leaf node exceeds n_thr, that node is expanded
       (all successor nodes that don't already exist are created and edges to
       those nodes are initialized).
    2. An evaluation of P_rl (weighted by the softmax temperature beta) is
       added to the policy queue.
    3. The edges are initialized with priors obtained using P_tree.
    4. Once P_rl has evaluated, the priors are overwritten with these values.

    n_thr is adjusted dynamically to ensure that the rate at which states are
    added to the policy queue matches the rate at which the GPUs evaluate the
    policy network.

    After a node is expanded, an edge is selected from it using the selection
    algorithm above.

  Evaluation and backup:
    When a leaf node is reached that should not be expanded:
    1. If V_rl has not been evaluated yet for this edge, add V_rl to the value
       network queue.
    2. Backup a virtual loss to this edge and all ancestor edges to temporarily
       discourage other threads from exploring this same path:
         N_r += n_vl
         W_r -= n_vl
    3. Perform a rollout using P_fast until a terminal state is reached and an
       outcome (z) is obtained.
    4. Backup the outcome to this edge and all ancestor edges, also undoing
       for the virtual loss:
         N_r += 1 - n_vl
         W_r += z + n_vl
    5. Once V_rl has been evaluated, backup the value to this edge and all
       ancestor edges:
         N_v += 1
         W_v += V_rl(s, a)

  After running a number of simulations, the action from the root node with the
  highest visit count is chosen. This is less sensitive to outliers than
  choosing the maximum Q(s, a).

  The subtree of the chosen action is reused (keeping existing statistics)
  while the nodes no longer reachable are discarded.

  The match version of AlphaGo continues searching during the opponent's move.
  It extends the search if the max visit count and the max Q(s, a) are for
  different actions. Time controls were otherwise shaped to use most time in
  the middle-game. AlphaGo resigns when its overall evaluation drops below an
  estimated 10% probability of winning the game: max Q(s, a) < −0.8

  For more details on the rollout policy and the optimizations used to perform
  rollouts, refer to the section in the paper titled "Rollout Policy."

  Distributed MCTS:
    * A single master machine executes the main search.
    * Many remote worker CPUs execute asynchronous rollouts.
    * Many remote worker GPUs execute asynchronous policy and value network
      evaluations.
    * The entire search tree is stored on the master, which only executes the
      in-tree phase of each simulation.
    * The leaf positions are communicated to the worker CPUs, which execute the
      rollout phase of the simulation, and to the worker GPUs, which compute
      network features and evaluate the policy and value networks.
    * The prior probabilities of the policy network are returned to the master,
      where they replace placeholder prior probabilities at the newly expanded
      node.
    * The rewards from rollouts and the value network outputs are each returned
      to the master, and backed up the originating search path.

  Notes:
    * During MCTS, P_rl and V_rl use a batch size of 1 to minimize evaluation
      time.
    * Since the game of Go is invariant under rotation and reflection, when the
      state is passed to P_rl or V_rl, a random rotation and reflection is
      chosen (the averages over all of them in the limit).

  Single machine MCTS:
    * 40 search threads
    * 48 CPUs
    * 8 GPUs

  Distributed MCTS:
    * 40 search threads
    * 1,202 CPUs
    * 176 GPUs

  Uncertainties:
    * When is Q(s, a) updated? It seems to be updated during every node step
      in the selection phase, but that seems inefficient. Maybe a dirty flag
      could be set at backup and then during selection it could only be updated
      when needed, but I'm not sure if this would be more efficient.

AlphaGo Versions:
  (source: https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ)

  AlphaGo Fan (detailed above):
    The program that played against Fan Hui in October 2015. This program was
    distributed over many machines using 176 GPUs.

  AlphaGo Lee:
    The program that defeated Lee Sedol 4–1 in March 2016. It is similar in
    most regards to AlphaGo Fan, with several key differences. First, the
    value network was trained from the outcomes of fast games of self-play by
    AlphaGo, rather than games of self-play by the policy network. Second, the
    policy and value networks were larger than those described above, using 12
    conv layers of 256 planes and were trained for more iterations. This
    player was also distributed over many machines using 48 TPUs, rather than
    GPUs, enabling it to evaluate neural networks faster during search.

  AlphaGo Master:
    The program that defeated top human players by 60–0 in January 2017. It
    was previously unpublished, but uses the same neural network architecture,
    reinforcement learning algorithm, and MCTS algorithm as described in the
    AlphaGo Zero paper. However, it uses the same handcrafted features and
    rollouts as AlphaGo Lee and training was initialized by supervised learning
    from human data.

  AlphaGo Zero:
    The program learns from self-play reinforcement learning, starting from
    random initial weights, without using rollouts, with no human supervision
    and using only the raw board history as input features. It uses just a
    single machine in the Google Cloud with 4 TPUs (AlphaGo Zero could also be
    distributed, but the researchers chose to use a simpler search algorithm).